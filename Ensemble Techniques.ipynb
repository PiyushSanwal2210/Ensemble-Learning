{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a860ad6",
   "metadata": {},
   "source": [
    "########## THEORY QUESTIONS ##########"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe26b3b",
   "metadata": {},
   "source": [
    "Question = 1 >>> What is Ensemble Learning in machine learning? Explain the key idea behind it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4cff5e9",
   "metadata": {},
   "source": [
    "Ans = Ensemble Learning in machine learning is a technique where multiple models (often called “weak learners”) are combined to create a more powerful model (a “strong learner”).\n",
    "\n",
    "The key idea behind ensemble learning is:\n",
    "### A group of weak models, when combined, can perform better than any single model alone.\n",
    "\n",
    "This works because different models may make different errors, and by aggregating their predictions, the ensemble reduces variance, bias, or improves generalization.\n",
    "\n",
    "Main Types of Ensemble Methods:\n",
    "\n",
    "Bagging (Bootstrap Aggregating)\n",
    "\n",
    "Multiple models are trained on different random subsets of the training data.\n",
    "\n",
    "Predictions are averaged (for regression) or voted (for classification).\n",
    "\n",
    "Example: Random Forest.\n",
    "\n",
    "Goal: Reduce variance (overfitting).\n",
    "\n",
    "Boosting\n",
    "\n",
    "Models are trained sequentially, each new model focusing on correcting the mistakes of the previous ones.\n",
    "\n",
    "Predictions are combined in a weighted manner.\n",
    "\n",
    "Example: AdaBoost, Gradient Boosting, XGBoost, LightGBM.\n",
    "\n",
    "Goal: Reduce bias (underfitting).\n",
    "\n",
    "Stacking (Stacked Generalization)\n",
    "\n",
    "Multiple models (level-0 learners) are trained, and their outputs are fed into a meta-model (level-1 learner), which makes the final prediction.\n",
    "\n",
    "Goal: Improve overall performance by leveraging strengths of different algorithms.\n",
    "\n",
    "### In summary:\n",
    "Ensemble learning mimics the idea of “wisdom of the crowd.” Instead of relying on a single model’s decision, it combines multiple models to get more accurate, stable, and robust predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1678d4bc",
   "metadata": {},
   "source": [
    "Question = 2 >>> What is the difference between Bagging and Boosting?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebb1a9c",
   "metadata": {},
   "source": [
    "Ans = 1. Core Idea\n",
    "\n",
    "Bagging (Bootstrap Aggregating):\n",
    "\n",
    "Trains models independently in parallel on different random subsets of the data (sampled with replacement).\n",
    "\n",
    "Then combines their predictions by averaging (regression) or majority vote (classification).\n",
    "\n",
    "Goal: Reduce variance (helps when models overfit).\n",
    "\n",
    "Boosting:\n",
    "\n",
    "Trains models sequentially. Each new model focuses more on the errors (misclassified points) made by the previous models.\n",
    "\n",
    "Predictions are combined in a weighted manner.\n",
    "\n",
    "Goal: Reduce bias (helps when models underfit).\n",
    "\n",
    "2. Model Training\n",
    "\n",
    "Bagging: All models are equal; trained independently.\n",
    "\n",
    "Boosting: Models are built one after another; later models depend on earlier ones.\n",
    "\n",
    "3. Weighting of Models/Data\n",
    "\n",
    "Bagging:\n",
    "\n",
    "Each model is given equal weight in final prediction.\n",
    "\n",
    "Each data sample has an equal chance of being chosen in subsets.\n",
    "\n",
    "Boosting:\n",
    "\n",
    "Models are given different weights (better-performing models get more influence).\n",
    "\n",
    "Misclassified samples get higher weights, so future models focus on them.\n",
    "\n",
    "4. Overfitting\n",
    "\n",
    "Bagging: Reduces overfitting (variance) but doesn’t reduce bias much.\n",
    "\n",
    "Boosting: Reduces bias, but may increase overfitting if not regularized properly.\n",
    "\n",
    "5. Examples\n",
    "\n",
    "Bagging: Random Forest, Bagged Decision Trees.\n",
    "\n",
    "Boosting: AdaBoost, Gradient Boosting, XGBoost, LightGBM, CatBoost.\n",
    "\n",
    "### In short:\n",
    "\n",
    "Bagging = Parallel, Equal weight, Reduce Variance (overfitting).\n",
    "\n",
    "Boosting = Sequential, Weighted, Reduce Bias (underfitting).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc2b9e8",
   "metadata": {},
   "source": [
    "Question = 3 >>> What is bootstrap sampling and what role does it play in Bagging methods like Random Forest?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9b8281",
   "metadata": {},
   "source": [
    "Ans = Bootstrap Sampling\n",
    "\n",
    "Definition:\n",
    "Bootstrap sampling is a technique where we create multiple random samples from the original dataset with replacement.\n",
    "\n",
    "\"With replacement\" means once a data point is picked, it is put back into the dataset before the next draw, so the same point can appear multiple times in a sample.\n",
    "\n",
    "Each bootstrap sample is the same size as the original dataset but contains repeated and missing observations.\n",
    "\n",
    "### Example:\n",
    "Original dataset = [1, 2, 3, 4]\n",
    "One bootstrap sample (with replacement) could be [2, 2, 3, 4] or [1, 3, 3, 4].\n",
    "\n",
    "Role in Bagging (e.g., Random Forest)\n",
    "\n",
    "In Bagging, the idea is to train multiple models on different versions of the dataset so they make diverse predictions.\n",
    "\n",
    "Bootstrap sampling provides these different versions:\n",
    "\n",
    "Diversity: Each model (say a decision tree in Random Forest) is trained on a different bootstrap sample, so they don’t see exactly the same data.\n",
    "\n",
    "Variance Reduction: By combining predictions (averaging or majority vote), the randomness reduces overfitting and stabilizes the model.\n",
    "\n",
    "Out-of-Bag (OOB) Estimate: Since some points are left out of each bootstrap sample (~36% on average), these unseen points can be used as a validation set to estimate model performance without needing a separate test set.\n",
    "\n",
    "In Random Forest specifically\n",
    "\n",
    "Each tree is trained on a bootstrap sample of the data.\n",
    "\n",
    "On top of that, Random Forest also adds another layer of randomness: when splitting a node, it considers only a random subset of features.\n",
    "\n",
    "Together, bootstrap sampling (data randomness) + feature randomness makes Random Forest a powerful, less correlated ensemble of trees.\n",
    "\n",
    "### In summary:\n",
    "Bootstrap sampling = drawing random subsets with replacement.\n",
    "In Bagging/Random Forest, it ensures diverse training data for each model, helping reduce variance, prevent overfitting, and allow OOB error estimation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d71eb1c",
   "metadata": {},
   "source": [
    "Question = 4 >>> What are Out-of-Bag (OOB) samples and how is OOB score used to evaluate ensemble models?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3fdfe75",
   "metadata": {},
   "source": [
    "Ans = What are Out-of-Bag (OOB) Samples?\n",
    "\n",
    "In bootstrap sampling (used in Bagging and Random Forest), each model is trained on a random sample with replacement.\n",
    "\n",
    "On average, each bootstrap sample contains about 63% of the original data, because some points are selected multiple times.\n",
    "\n",
    "The remaining ~37% of data that is not included in that bootstrap sample is called the Out-of-Bag (OOB) samples.\n",
    "\n",
    "### Intuition:\n",
    "If you flip a coin (pick a sample) many times with replacement, the chance a specific point is never picked in a sample of size N is about \n",
    "(\n",
    "1\n",
    "−\n",
    "1\n",
    "𝑁\n",
    ")\n",
    "𝑁\n",
    "≈\n",
    "𝑒\n",
    "−\n",
    "1\n",
    "≈\n",
    "0.368\n",
    "(1−\n",
    "N\n",
    "1\n",
    "\t​\n",
    "\n",
    ")\n",
    "N\n",
    "≈e\n",
    "−1\n",
    "≈0.368 → 36.8% left out.\n",
    "\n",
    "How OOB Score is Used\n",
    "\n",
    "After training a model (say a tree) on its bootstrap sample, we can test it on the data points it did not see (the OOB samples).\n",
    "\n",
    "Each data point may be left out of multiple bootstrap samples, so it will have predictions from several models.\n",
    "\n",
    "The OOB score is computed by aggregating these predictions and comparing them with the true labels.\n",
    "\n",
    "For Classification:\n",
    "\n",
    "Each data point is classified using the majority vote of the models for which it was OOB.\n",
    "\n",
    "The OOB accuracy = fraction of correctly classified OOB samples.\n",
    "\n",
    "For Regression:\n",
    "\n",
    "Each data point’s prediction is the average of the models where it was OOB.\n",
    "\n",
    "OOB error = mean squared error (MSE) or another regression metric.\n",
    "\n",
    "Why is OOB Score Useful?\n",
    "\n",
    "### Acts like an internal cross-validation, so no need for a separate validation set.\n",
    "### Provides an unbiased estimate of test error (since OOB samples are unseen during training).\n",
    "### Saves data, especially valuable when dataset is small.\n",
    "\n",
    "Example (Random Forest):\n",
    "\n",
    "Train 100 trees on bootstrap samples.\n",
    "\n",
    "For each sample \n",
    "𝑥\n",
    "𝑖\n",
    "x\n",
    "i\n",
    "\t​\n",
    "\n",
    ", about 37 trees (on average) won’t have seen it.\n",
    "\n",
    "Use those 37 trees’ predictions to estimate \n",
    "𝑥\n",
    "𝑖\n",
    "x\n",
    "i\n",
    "\t​\n",
    "\n",
    "’s label.\n",
    "\n",
    "Compare with the true label → contributes to OOB accuracy.\n",
    "\n",
    "### In summary:\n",
    "\n",
    "OOB samples = data not included in a bootstrap sample.\n",
    "\n",
    "OOB score = performance of the ensemble on these OOB samples, giving a built-in, reliable estimate of generalization error.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c0713a",
   "metadata": {},
   "source": [
    "Question = 5 >>> Compare feature importance analysis in a single Decision Tree vs. a Random Forest.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6159d5f4",
   "metadata": {},
   "source": [
    "Ans = 1. Decision Tree – Feature Importance\n",
    "\n",
    "A single Decision Tree determines feature importance based on how much each feature reduces impurity (e.g., Gini index, entropy, or variance) when used for splitting.\n",
    "\n",
    "For each split:\n",
    "\n",
    "Importance(feature) = \\sum \\text{(impurity decrease)} \\times \\frac{\\text{# samples in node}}{\\text{total samples}}\n",
    "\n",
    "The final importance is normalized so all feature importances sum to 1.\n",
    "\n",
    "### Limitation:\n",
    "\n",
    "Sensitive to noise.\n",
    "\n",
    "A tree can overfit and give high importance to irrelevant features.\n",
    "\n",
    "Importance is biased toward features with more categories or continuous ranges.\n",
    "\n",
    "2. Random Forest – Feature Importance\n",
    "\n",
    "A Random Forest is an ensemble of many decision trees, each trained on a bootstrap sample + random subset of features at splits.\n",
    "\n",
    "Feature importance is calculated by averaging the importance of each feature across all trees in the forest.\n",
    "\n",
    "This reduces variance and bias compared to a single tree.\n",
    "\n",
    "### Advantages over a single tree:\n",
    "\n",
    "More stable: Randomization + averaging smooth out noise.\n",
    "\n",
    "Less biased: Because not all features are considered at every split, importance isn’t dominated by a single strong predictor.\n",
    "\n",
    "More reliable: Works better in high-dimensional settings.\n",
    "\n",
    "Comparison Table\n",
    "Aspect\tDecision Tree\tRandom Forest\n",
    "How importance is computed\tBased on impurity reduction in that one tree\tAveraged impurity reduction across all trees\n",
    "Stability\tUnstable; small changes in data can change importance drastically\tStable; averaging across many trees reduces variance\n",
    "Bias toward features\tBiased toward features with many categories or continuous values\tBias reduced (due to feature randomness in splits)\n",
    "Overfitting risk\tHigh (importance may reflect noise)\tLower (averaging helps generalization)\n",
    "Interpretability\tEasy to interpret, but may be misleading\tHarder to interpret, but more reliable\n",
    "\n",
    "### In summary:\n",
    "\n",
    "Decision Tree feature importance = impurity reduction in one model (simple but unstable).\n",
    "\n",
    "Random Forest feature importance = averaged importance across many trees (more stable, reliable, and less biased).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9730acc2",
   "metadata": {},
   "source": [
    "Question = 6 >>> Write a Python program to:\n",
    "● Load the Breast Cancer dataset using\n",
    "sklearn.datasets.load_breast_cancer()\n",
    "● Train a Random Forest Classifier\n",
    "● Print the top 5 most important features based on feature importance scores.\n",
    "(Include your Python code and output in the code box below.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f4374033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 Most Important Features:\n",
      "\n",
      "             Feature  Importance\n",
      "          worst area    0.139357\n",
      "worst concave points    0.132225\n",
      " mean concave points    0.107046\n",
      "        worst radius    0.082848\n",
      "     worst perimeter    0.080850\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "feature_names = data.feature_names\n",
    "\n",
    "# Train Random Forest Classifier\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42, oob_score=True)\n",
    "rf.fit(X, y)\n",
    "\n",
    "# Get feature importance scores\n",
    "importances = rf.feature_importances_\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    \"Feature\": feature_names,\n",
    "    \"Importance\": importances\n",
    "})\n",
    "\n",
    "# Sort by importance and get top 5\n",
    "top_features = feature_importance_df.sort_values(by=\"Importance\", ascending=False).head(5)\n",
    "\n",
    "# Print the results\n",
    "print(\"Top 5 Most Important Features:\\n\")\n",
    "print(top_features.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f65cbac",
   "metadata": {},
   "source": [
    "Question = 7 >>> Write a Python program to:\n",
    "● Train a Bagging Classifier using Decision Trees on the Iris dataset\n",
    "● Evaluate its accuracy and compare with a single Decision Tree\n",
    "(Include your Python code and output in the code box below.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cc5223b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Comparison on Iris Dataset:\n",
      "\n",
      "Single Decision Tree Accuracy: 0.9333\n",
      "Bagging Classifier Accuracy : 0.9333\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Single Decision Tree\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "dt.fit(X_train, y_train)\n",
    "y_pred_dt = dt.predict(X_test)\n",
    "dt_accuracy = accuracy_score(y_test, y_pred_dt)\n",
    "\n",
    "# Bagging Classifier with Decision Trees\n",
    "bagging = BaggingClassifier(\n",
    "    estimator=DecisionTreeClassifier(),\n",
    "    n_estimators=50,  # number of trees\n",
    "    random_state=42\n",
    ")\n",
    "bagging.fit(X_train, y_train)\n",
    "y_pred_bag = bagging.predict(X_test)\n",
    "bagging_accuracy = accuracy_score(y_test, y_pred_bag)\n",
    "\n",
    "# Print results\n",
    "print(\"Accuracy Comparison on Iris Dataset:\\n\")\n",
    "print(f\"Single Decision Tree Accuracy: {dt_accuracy:.4f}\")\n",
    "print(f\"Bagging Classifier Accuracy : {bagging_accuracy:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2005713b",
   "metadata": {},
   "source": [
    "Question = 8 >>> : Write a Python program to:\n",
    "● Train a Random Forest Classifier\n",
    "● Tune hyperparameters max_depth and n_estimators using GridSearchCV\n",
    "● Print the best parameters and final accuracy\n",
    "(Include your Python code and output in the code box below.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c8c97a9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters Found:\n",
      " {'max_depth': None, 'n_estimators': 100}\n",
      "\n",
      "Final Accuracy on Test Set: 0.9357\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load dataset\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Define Random Forest Classifier\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 5, 10, 20]\n",
    "}\n",
    "\n",
    "# GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=rf,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,                # 5-fold cross-validation\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1\n",
    ")\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Evaluate on test set\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "final_accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Print results\n",
    "print(\"Best Parameters Found:\\n\", best_params)\n",
    "print(f\"\\nFinal Accuracy on Test Set: {final_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66928c9e",
   "metadata": {},
   "source": [
    "Question = 9 >>> Write a Python program to:\n",
    "● Train a Bagging Regressor and a Random Forest Regressor on the California\n",
    "Housing dataset\n",
    "● Compare their Mean Squared Errors (MSE)\n",
    "(Include your Python code and output in the code box below.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2b7aae18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error Comparison:\n",
      "\n",
      "Bagging Regressor MSE      : 0.2579\n",
      "Random Forest Regressor MSE: 0.2577\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "os.environ[\"LOKY_MAX_CPU_COUNT\"] = \"1\"\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load California Housing dataset\n",
    "housing = fetch_california_housing()\n",
    "X, y = housing.data, housing.target\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Bagging Regressor (with Decision Trees)\n",
    "bagging_reg = BaggingRegressor(\n",
    "    estimator=DecisionTreeRegressor(),\n",
    "    n_estimators=50,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "bagging_reg.fit(X_train, y_train)\n",
    "y_pred_bag = bagging_reg.predict(X_test)\n",
    "mse_bagging = mean_squared_error(y_test, y_pred_bag)\n",
    "\n",
    "# Random Forest Regressor\n",
    "rf_reg = RandomForestRegressor(\n",
    "    n_estimators=50,\n",
    "    random_state=42,\n",
    "    n_jobs=1\n",
    ")\n",
    "rf_reg.fit(X_train, y_train)\n",
    "y_pred_rf = rf_reg.predict(X_test)\n",
    "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
    "\n",
    "# Print results\n",
    "print(\"Mean Squared Error Comparison:\\n\")\n",
    "print(f\"Bagging Regressor MSE      : {mse_bagging:.4f}\")\n",
    "print(f\"Random Forest Regressor MSE: {mse_rf:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3ca9bb",
   "metadata": {},
   "source": [
    "Question = 10 >>> You are working as a data scientist at a financial institution to predict loan\n",
    "default. You have access to customer demographic and transaction history data.\n",
    "You decide to use ensemble techniques to increase model performance.\n",
    "Explain your step-by-step approach to:\n",
    "● Choose between Bagging or Boosting\n",
    "● Handle overfitting\n",
    "● Select base models\n",
    "● Evaluate performance using cross-validation\n",
    "● Justify how ensemble learning improves decision-making in this real-world\n",
    "context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664c2649",
   "metadata": {},
   "source": [
    "Ans = Here’s a crisp, practical plan tailored to loan-default prediction with demographics + transaction history.\n",
    "\n",
    "1) Choose between Bagging vs Boosting\n",
    "\n",
    "Start with diagnostics on a simple baseline (regularized logistic or shallow tree).\n",
    "\n",
    "If training accuracy >> CV accuracy (high variance) → prefer Bagging/Random Forest to stabilize.\n",
    "\n",
    "If both train and CV accuracies are modest (high bias), complex interactions evident → prefer Boosting (XGBoost/LightGBM/CatBoost).\n",
    "\n",
    "Data traits guide the choice:\n",
    "\n",
    "Many weak, noisy predictors; need stability & OOB estimate → Random Forest.\n",
    "\n",
    "Strong non-linearities, heterogeneous segments, class imbalance, need top-rank lift → Boosting.\n",
    "\n",
    "Many categorical variables (e.g., merchant categories) → CatBoost (handles categoricals natively) or LightGBM with careful encoding.\n",
    "\n",
    "Reality: try both under the same time-aware CV and pick by PR-AUC / KS / Log-loss (not accuracy), then calibrate.\n",
    "\n",
    "2) Handle Overfitting\n",
    "\n",
    "Data/Leakage controls\n",
    "\n",
    "Use a time-aware split (train on older months → validate on newer) to reflect deployment.\n",
    "\n",
    "Group by customer_id to keep all records for a customer in the same fold.\n",
    "\n",
    "Build features only from information available before the decision time (no post-application signals).\n",
    "\n",
    "Bagging/Random Forest\n",
    "\n",
    "Limit tree complexity: max_depth, min_samples_leaf, max_features (sqrt/log2).\n",
    "\n",
    "Use many trees; monitor OOB error for a cheap generalization check.\n",
    "\n",
    "Boosting\n",
    "\n",
    "Small learning_rate, early stopping on a validation fold, shallow trees (max_depth 3–8).\n",
    "\n",
    "Subsampling rows/columns (subsample, colsample_bytree) and regularization (lambda, alpha).\n",
    "\n",
    "Consider monotonic constraints for known relationships (e.g., higher DPD → higher risk).\n",
    "\n",
    "Post-model\n",
    "\n",
    "Calibrate probabilities (Platt/Isotonic) for reliable PDs used in risk and pricing.\n",
    "\n",
    "Control target leakage in encodings (use K-fold target encoding with out-of-fold scheme).\n",
    "\n",
    "3) Select Base Models\n",
    "\n",
    "Bagging: high-variance base learners (unpruned or moderately deep decision trees). Use Random Forest or ExtraTrees as strong bagging baselines.\n",
    "\n",
    "Boosting: gradient-boosted trees (XGBoost/LightGBM/CatBoost) with shallow trees (stumps to depth~6).\n",
    "\n",
    "Feature types\n",
    "\n",
    "Numeric + many one-hots → LightGBM/XGBoost.\n",
    "\n",
    "Many raw categoricals with high cardinality → CatBoost (less preprocessing).\n",
    "\n",
    "Benchmark for governance: include penalized logistic regression (interpretable) and optionally a stacked ensemble (meta-learner = logistic on out-of-fold predictions) if your governance allows.\n",
    "\n",
    "4) Evaluate with Cross-Validation (risk-appropriate)\n",
    "\n",
    "CV design\n",
    "\n",
    "TimeSeriesSplit / rolling origin CV (e.g., 5 folds by month/quarter).\n",
    "\n",
    "Stratify by default rate within time blocks if feasible; always group by customer.\n",
    "\n",
    "Primary metrics\n",
    "\n",
    "PR-AUC (class imbalance), ROC-AUC, KS, Log-loss/Brier (probability quality).\n",
    "\n",
    "Calibration: reliability curve; ECE/Brier.\n",
    "\n",
    "Business metrics: expected profit/cost at chosen threshold, bad-rate at fixed approval rate.\n",
    "\n",
    "Choose threshold \n",
    "𝑡\n",
    "t to maximize:\n",
    "\n",
    "Expected Profit\n",
    "(\n",
    "𝑡\n",
    ")\n",
    "=\n",
    "∑\n",
    "𝑖\n",
    "[\n",
    "1\n",
    "(\n",
    "𝑝\n",
    "^\n",
    "𝑖\n",
    "<\n",
    "𝑡\n",
    ")\n",
    "⋅\n",
    "margin\n",
    "𝑖\n",
    "−\n",
    "1\n",
    "(\n",
    "𝑝\n",
    "^\n",
    "𝑖\n",
    "≥\n",
    "𝑡\n",
    ")\n",
    "⋅\n",
    "LGD\n",
    "𝑖\n",
    "⋅\n",
    "EAD\n",
    "𝑖\n",
    "]\n",
    "Expected Profit(t)=∑\n",
    "i\n",
    "\t​\n",
    "\n",
    "[1(\n",
    "p\n",
    "^\n",
    "\t​\n",
    "\n",
    "i\n",
    "\t​\n",
    "\n",
    "<t)⋅margin\n",
    "i\n",
    "\t​\n",
    "\n",
    "−1(\n",
    "p\n",
    "^\n",
    "\t​\n",
    "\n",
    "i\n",
    "\t​\n",
    "\n",
    "≥t)⋅LGD\n",
    "i\n",
    "\t​\n",
    "\n",
    "⋅EAD\n",
    "i\n",
    "\t​\n",
    "\n",
    "]\n",
    "or minimize cost \n",
    "𝐶\n",
    "=\n",
    "𝑐\n",
    "𝐹\n",
    "𝑃\n",
    "⋅\n",
    "𝐹\n",
    "𝑃\n",
    "+\n",
    "𝑐\n",
    "𝐹\n",
    "𝑁\n",
    "⋅\n",
    "𝐹\n",
    "𝑁\n",
    "C=c\n",
    "FP\n",
    "\t​\n",
    "\n",
    "⋅FP+c\n",
    "FN\n",
    "\t​\n",
    "\n",
    "⋅FN.\n",
    "\n",
    "Validation protocol\n",
    "\n",
    "Hyperparameter tuning inside each training window (nested CV or a fixed validation month with early stopping).\n",
    "\n",
    "Final model refit on train+val (past data), locked test month for last check.\n",
    "\n",
    "For RF, compare CV with OOB as a sanity check.\n",
    "\n",
    "5) Why ensemble learning improves decisions here\n",
    "\n",
    "Higher rank-ordering power: Boosting/RF typically lift PR-AUC/KS, giving cleaner separation of good vs bad—lets credit policy set smarter cutoffs at the same approval rate (or higher approvals at same risk).\n",
    "\n",
    "Robustness & stability: Bagging decorrelates trees; Boosting with regularization captures complex patterns without exploding variance. More stable PDs → steadier capital/ECL estimates and pricing.\n",
    "\n",
    "Feature interactions for free: Trees handle non-linearities and interactions common in transaction behavior (spikes, recency, volatility) without manual engineering.\n",
    "\n",
    "Actionable explanations: Use SHAP (global & local) for regulator-friendly reason codes, segment analysis, and bias checks.\n",
    "\n",
    "Operational impact: Better PD calibration → improved limit setting, pricing/interest tiering, collections prioritization, and fraud-risk triage; portfolio simulations show gains in profit per booked loan and loss rate reductions.\n",
    "\n",
    "Quick recipe you can run\n",
    "\n",
    "Build a time-aware feature set (recency/frequency/monetary, delinquencies, utilization trends).\n",
    "\n",
    "Train Random Forest (variance control) and LightGBM/CatBoost (bias control) under the same rolling CV.\n",
    "\n",
    "Pick the winner by PR-AUC + calibration + profit at policy threshold.\n",
    "\n",
    "Calibrate, set threshold by expected profit/cost, and run backtests.\n",
    "\n",
    "Validate with stability (PSI), fairness, and reason codes; productionize with monitoring for drift and retraining cadence.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
